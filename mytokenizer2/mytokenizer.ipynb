{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[token(string='Influence', tag=None, offset=(0, 9))]\n",
      "[token(string=' ', tag=['SP'], offset=(9, 10))]\n",
      "[token(string='of', tag=None, offset=(10, 12))]\n",
      "[token(string=' ', tag=['SP'], offset=(12, 13))]\n",
      "[token(string='magnetic', tag=None, offset=(13, 21))]\n",
      "[token(string=' ', tag=['SP'], offset=(21, 22))]\n",
      "[token(string='frustration', tag=None, offset=(22, 33))]\n",
      "[token(string=' ', tag=['SP'], offset=(33, 34))]\n",
      "[token(string='and', tag=None, offset=(34, 37))]\n",
      "[token(string=' ', tag=['SP'], offset=(37, 38))]\n",
      "[token(string='structural', tag=None, offset=(38, 48))]\n",
      "[token(string=' ', tag=['SP'], offset=(48, 49))]\n",
      "[token(string='disorder', tag=None, offset=(49, 57))]\n",
      "[token(string=' ', tag=['SP'], offset=(57, 58))]\n",
      "[token(string='on', tag=None, offset=(58, 60))]\n",
      "[token(string=' ', tag=['SP'], offset=(60, 61))]\n",
      "[token(string='magnetocaloric', tag=None, offset=(61, 75))]\n",
      "[token(string=' ', tag=['SP'], offset=(75, 76))]\n",
      "[token(string='effect', tag=None, offset=(76, 82))]\n",
      "[token(string=' ', tag=['SP'], offset=(82, 83))]\n",
      "[token(string='and', tag=None, offset=(83, 86))]\n",
      "[token(string=' ', tag=['SP'], offset=(86, 87))]\n",
      "[token(string='transport', tag=None, offset=(8, 17))]\n",
      "[token(string=' ', tag=['SP'], offset=(104, 105))]\n",
      "[token(string='properties', tag=None, offset=(105, 115))]\n",
      "[token(string=' ', tag=['SP'], offset=(115, 116))]\n",
      "[token(string='in', tag=None, offset=(116, 118))]\n",
      "[token(string=' ', tag=['SP'], offset=(118, 119))]\n",
      "[token(string='La', tag=None, offset=(119, 121))]\n",
      "[token(string='1.5', tag=[key(tag='sub', attrs={})], offset=(121, 124))]\n",
      "[token(string='Ca', tag=None, offset=(124, 126))]\n",
      "[token(string='0.5', tag=[key(tag='sub', attrs={})], offset=(126, 129))]\n",
      "[token(string='CoMnO', tag=None, offset=(129, 134))]\n",
      "[token(string='6', tag=[key(tag='sub', attrs={})], offset=(134, 135))]\n",
      "[token(string=' ', tag=['SP'], offset=(135, 136))]\n",
      "[token(string='double', tag=None, offset=(136, 142))]\n",
      "[token(string=' ', tag=['SP'], offset=(142, 143))]\n",
      "[token(string='perovskite', tag=None, offset=(143, 153))]\n",
      "[token(string='\\n', tag=['SP'], offset=(153, 154))]\n",
      "0 41\n",
      "[token(string='The', tag=None, offset=(154, 157))]\n",
      "[token(string=' ', tag=['SP'], offset=(157, 158))]\n",
      "[token(string='rare', tag=None, offset=(158, 162))]\n",
      "[token(string=' ', tag=['SP'], offset=(162, 163))]\n",
      "[token(string='existence', tag=None, offset=(163, 172))]\n",
      "[token(string=' ', tag=['SP'], offset=(172, 173))]\n",
      "[token(string='of', tag=None, offset=(173, 175))]\n",
      "[token(string=' ', tag=['SP'], offset=(175, 176))]\n",
      "[token(string='a', tag=None, offset=(176, 177))]\n",
      "[token(string=' ', tag=['SP'], offset=(177, 178))]\n",
      "[token(string='magnetocaloric', tag=None, offset=(178, 192))]\n",
      "[token(string=' ', tag=['SP'], offset=(192, 193))]\n",
      "[token(string='effect,', tag=None, offset=(193, 200))]\n",
      "[token(string=' ', tag=['SP'], offset=(200, 201))]\n",
      "[token(string='a', tag=None, offset=(201, 202))]\n",
      "[token(string=' ', tag=['SP'], offset=(202, 203))]\n",
      "[token(string='Griffith', tag=None, offset=(203, 211))]\n",
      "[token(string=' ', tag=['SP'], offset=(211, 212))]\n",
      "[token(string='phase,', tag=None, offset=(212, 218))]\n",
      "[token(string=' ', tag=['SP'], offset=(218, 219))]\n",
      "[token(string='and', tag=None, offset=(219, 222))]\n",
      "[token(string=' ', tag=['SP'], offset=(222, 223))]\n",
      "[token(string='frustrated', tag=None, offset=(223, 233))]\n",
      "[token(string=' ', tag=['SP'], offset=(233, 234))]\n",
      "[token(string='magnetism', tag=None, offset=(234, 243))]\n",
      "[token(string=' ', tag=['SP'], offset=(243, 244))]\n",
      "[token(string='in', tag=None, offset=(244, 246))]\n",
      "[token(string=' ', tag=['SP'], offset=(246, 247))]\n",
      "[token(string='the', tag=None, offset=(247, 250))]\n",
      "[token(string=' ', tag=['SP'], offset=(250, 251))]\n",
      "[token(string='antisite', tag=None, offset=(251, 259))]\n",
      "[token(string=' ', tag=['SP'], offset=(259, 260))]\n",
      "[token(string='disorder', tag=None, offset=(260, 268))]\n",
      "[token(string=' ', tag=['SP'], offset=(268, 269))]\n",
      "[token(string='compound', tag=None, offset=(269, 277))]\n",
      "[token(string=' ', tag=['SP'], offset=(277, 278))]\n",
      "[token(string='La', tag=None, offset=(278, 280))]\n",
      "[token(string='1.5', tag=[key(tag='sub', attrs={})], offset=(280, 283))]\n",
      "[token(string='Ca', tag=None, offset=(283, 285))]\n",
      "[token(string='0.5', tag=[key(tag='sub', attrs={})], offset=(285, 288))]\n",
      "[token(string='CoMnO', tag=None, offset=(288, 293))]\n",
      "[token(string='6', tag=[key(tag='sub', attrs={})], offset=(293, 294))]\n",
      "[token(string=' ', tag=['SP'], offset=(294, 295))]\n",
      "[token(string='have', tag=None, offset=(295, 299))]\n",
      "[token(string=' ', tag=['SP'], offset=(299, 300))]\n",
      "[token(string='been', tag=None, offset=(300, 304))]\n",
      "[token(string=' ', tag=['SP'], offset=(304, 305))]\n",
      "[token(string='investigated', tag=None, offset=(305, 317))]\n",
      "[token(string=' ', tag=['SP'], offset=(317, 318))]\n",
      "[token(string='in', tag=None, offset=(318, 320))]\n",
      "[token(string=' ', tag=['SP'], offset=(320, 321))]\n",
      "[token(string='detail', tag=None, offset=(321, 327))]\n",
      "[token(string=' ', tag=['SP'], offset=(327, 328))]\n",
      "[token(string='in', tag=None, offset=(328, 330))]\n",
      "[token(string=' ', tag=['SP'], offset=(330, 331))]\n",
      "[token(string='this', tag=None, offset=(331, 335))]\n",
      "[token(string=' ', tag=['SP'], offset=(335, 336))]\n",
      "[token(string='work.', tag=None, offset=(336, 341))]\n",
      "[token(string=' ', tag=['SP'], offset=(341, 342))]\n",
      "[token(string='The', tag=None, offset=(342, 345))]\n",
      "[token(string=' ', tag=['SP'], offset=(345, 346))]\n",
      "[token(string='nature', tag=None, offset=(346, 352))]\n",
      "[token(string=' ', tag=['SP'], offset=(352, 353))]\n",
      "[token(string='of', tag=None, offset=(353, 355))]\n",
      "[token(string=' ', tag=['SP'], offset=(355, 356))]\n",
      "[token(string='the', tag=None, offset=(356, 359))]\n",
      "[token(string=' ', tag=['SP'], offset=(359, 360))]\n",
      "[token(string='observed', tag=None, offset=(360, 368))]\n",
      "[token(string=' ', tag=['SP'], offset=(368, 369))]\n",
      "[token(string='Griffith', tag=None, offset=(369, 377))]\n",
      "[token(string=' ', tag=['SP'], offset=(377, 378))]\n",
      "[token(string='phase', tag=None, offset=(378, 383))]\n",
      "[token(string=' ', tag=['SP'], offset=(383, 384))]\n",
      "[token(string='(at', tag=None, offset=(384, 387))]\n",
      "[token(string=' ', tag=['SP'], offset=(387, 388))]\n",
      "[token(string='T', tag=None, offset=(388, 389))]\n",
      "[token(string='G', tag=[key(tag='sub', attrs={})], offset=(389, 390))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(390, 391))]\n",
      "[token(string='∼', tag=None, offset=(391, 392))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(392, 393))]\n",
      "[token(string='226', tag=None, offset=(393, 396))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(396, 397))]\n",
      "[token(string='K)', tag=None, offset=(397, 399))]\n",
      "[token(string=' ', tag=['SP'], offset=(399, 400))]\n",
      "[token(string='can', tag=None, offset=(400, 403))]\n",
      "[token(string=' ', tag=['SP'], offset=(403, 404))]\n",
      "[token(string='be', tag=None, offset=(404, 406))]\n",
      "[token(string=' ', tag=['SP'], offset=(406, 407))]\n",
      "[token(string='best', tag=None, offset=(407, 411))]\n",
      "[token(string=' ', tag=['SP'], offset=(411, 412))]\n",
      "[token(string='understood', tag=None, offset=(412, 422))]\n",
      "[token(string=' ', tag=['SP'], offset=(422, 423))]\n",
      "[token(string='in', tag=None, offset=(423, 425))]\n",
      "[token(string=' ', tag=['SP'], offset=(425, 426))]\n",
      "[token(string='terms', tag=None, offset=(426, 431))]\n",
      "[token(string=' ', tag=['SP'], offset=(431, 432))]\n",
      "[token(string='of', tag=None, offset=(432, 434))]\n",
      "[token(string=' ', tag=['SP'], offset=(434, 435))]\n",
      "[token(string='ferromagnetic', tag=None, offset=(435, 448))]\n",
      "[token(string=' ', tag=['SP'], offset=(448, 449))]\n",
      "[token(string='(FM)', tag=None, offset=(449, 453))]\n",
      "[token(string=' ', tag=['SP'], offset=(453, 454))]\n",
      "[token(string='entities', tag=None, offset=(454, 462))]\n",
      "[token(string=' ', tag=['SP'], offset=(462, 463))]\n",
      "[token(string='within', tag=None, offset=(463, 469))]\n",
      "[token(string=' ', tag=['SP'], offset=(469, 470))]\n",
      "[token(string='the', tag=None, offset=(470, 473))]\n",
      "[token(string=' ', tag=['SP'], offset=(473, 474))]\n",
      "[token(string='globally', tag=None, offset=(474, 482))]\n",
      "[token(string=' ', tag=['SP'], offset=(482, 483))]\n",
      "[token(string='paramagnetic', tag=None, offset=(483, 495))]\n",
      "[token(string=' ', tag=['SP'], offset=(495, 496))]\n",
      "[token(string='network', tag=None, offset=(496, 503))]\n",
      "[token(string=' ', tag=['SP'], offset=(503, 504))]\n",
      "[token(string='above', tag=None, offset=(504, 509))]\n",
      "[token(string=' ', tag=['SP'], offset=(509, 510))]\n",
      "[token(string='the', tag=None, offset=(510, 513))]\n",
      "[token(string=' ', tag=['SP'], offset=(513, 514))]\n",
      "[token(string='Curie', tag=None, offset=(514, 519))]\n",
      "[token(string=' ', tag=['SP'], offset=(519, 520))]\n",
      "[token(string='temperature.', tag=None, offset=(520, 532))]\n",
      "[token(string=' ', tag=['SP'], offset=(532, 533))]\n",
      "[token(string='From', tag=None, offset=(533, 537))]\n",
      "[token(string=' ', tag=['SP'], offset=(537, 538))]\n",
      "[token(string='the', tag=None, offset=(538, 541))]\n",
      "[token(string=' ', tag=['SP'], offset=(541, 542))]\n",
      "[token(string='isothermal', tag=None, offset=(542, 552))]\n",
      "[token(string=' ', tag=['SP'], offset=(552, 553))]\n",
      "[token(string='magnetization', tag=None, offset=(553, 566))]\n",
      "[token(string=' ', tag=['SP'], offset=(566, 567))]\n",
      "[token(string='measurement', tag=None, offset=(567, 578))]\n",
      "[token(string=' ', tag=['SP'], offset=(578, 579))]\n",
      "[token(string='around', tag=None, offset=(579, 585))]\n",
      "[token(string=' ', tag=['SP'], offset=(585, 586))]\n",
      "[token(string='Curie', tag=None, offset=(586, 591))]\n",
      "[token(string=' ', tag=['SP'], offset=(591, 592))]\n",
      "[token(string='temperature', tag=None, offset=(592, 603))]\n",
      "[token(string=' ', tag=['SP'], offset=(603, 604))]\n",
      "[token(string='(T', tag=None, offset=(604, 606))]\n",
      "[token(string='C', tag=[key(tag='sub', attrs={})], offset=(606, 607))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(607, 608))]\n",
      "[token(string='∼', tag=None, offset=(608, 609))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(609, 610))]\n",
      "[token(string='157', tag=None, offset=(610, 613))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(613, 614))]\n",
      "[token(string='K)', tag=None, offset=(614, 616))]\n",
      "[token(string=' ', tag=['SP'], offset=(616, 617))]\n",
      "[token(string='and', tag=None, offset=(617, 620))]\n",
      "[token(string=' ', tag=['SP'], offset=(620, 621))]\n",
      "[token(string='cluster', tag=None, offset=(621, 628))]\n",
      "[token(string=' ', tag=['SP'], offset=(628, 629))]\n",
      "[token(string='glass', tag=None, offset=(629, 634))]\n",
      "[token(string=' ', tag=['SP'], offset=(634, 635))]\n",
      "[token(string='transition', tag=None, offset=(635, 645))]\n",
      "[token(string=' ', tag=['SP'], offset=(645, 646))]\n",
      "[token(string='temperature', tag=None, offset=(646, 657))]\n",
      "[token(string=' ', tag=['SP'], offset=(657, 658))]\n",
      "[token(string='(T', tag=None, offset=(658, 660))]\n",
      "[token(string='g', tag=[key(tag='sub', attrs={})], offset=(660, 661))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(661, 662))]\n",
      "[token(string='∼', tag=None, offset=(662, 663))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(663, 664))]\n",
      "[token(string='51', tag=None, offset=(664, 666))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(666, 667))]\n",
      "[token(string='K),', tag=None, offset=(667, 670))]\n",
      "[token(string=' ', tag=['SP'], offset=(670, 671))]\n",
      "[token(string='we', tag=None, offset=(671, 673))]\n",
      "[token(string=' ', tag=['SP'], offset=(673, 674))]\n",
      "[token(string='have', tag=None, offset=(674, 678))]\n",
      "[token(string=' ', tag=['SP'], offset=(678, 679))]\n",
      "[token(string='determined', tag=None, offset=(679, 689))]\n",
      "[token(string=' ', tag=['SP'], offset=(689, 690))]\n",
      "[token(string='the', tag=None, offset=(690, 693))]\n",
      "[token(string=' ', tag=['SP'], offset=(693, 694))]\n",
      "[token(string='maximum', tag=None, offset=(694, 701))]\n",
      "[token(string=' ', tag=['SP'], offset=(701, 702))]\n",
      "[token(string='entropy', tag=None, offset=(702, 709))]\n",
      "[token(string=' ', tag=['SP'], offset=(709, 710))]\n",
      "[token(string='change', tag=None, offset=(710, 716))]\n",
      "[token(string=' ', tag=['SP'], offset=(716, 717))]\n",
      "[token(string='(−ΔS', tag=None, offset=(717, 721))]\n",
      "[token(string='M', tag=[key(tag='sub', attrs={})], offset=(721, 722))]\n",
      "[token(string=')', tag=None, offset=(722, 723))]\n",
      "[token(string=' ', tag=['SP'], offset=(723, 724))]\n",
      "[token(string='as', tag=None, offset=(724, 726))]\n",
      "[token(string=' ', tag=['SP'], offset=(726, 727))]\n",
      "[token(string='∼2.2', tag=None, offset=(727, 731))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(731, 732))]\n",
      "[token(string='kg', tag=None, offset=(2, 4))]\n",
      "[token(string=' ', tag=['SP'], offset=(736, 737))]\n",
      "[token(string='K', tag=None, offset=(737, 738))]\n",
      "[token(string=' ', tag=['SP'], offset=(738, 739))]\n",
      "[token(string='and', tag=None, offset=(739, 742))]\n",
      "[token(string=' ', tag=['SP'], offset=(742, 743))]\n",
      "[token(string='∼1.2', tag=None, offset=(743, 747))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(747, 748))]\n",
      "[token(string='kg', tag=None, offset=(2, 4))]\n",
      "[token(string=' ', tag=['SP'], offset=(752, 753))]\n",
      "[token(string='K,', tag=None, offset=(753, 755))]\n",
      "[token(string=' ', tag=['SP'], offset=(755, 756))]\n",
      "[token(string='respectively,', tag=None, offset=(756, 769))]\n",
      "[token(string=' ', tag=['SP'], offset=(769, 770))]\n",
      "[token(string='for', tag=None, offset=(770, 773))]\n",
      "[token(string=' ', tag=['SP'], offset=(773, 774))]\n",
      "[token(string='a', tag=None, offset=(774, 775))]\n",
      "[token(string=' ', tag=['SP'], offset=(775, 776))]\n",
      "[token(string='magnetic', tag=None, offset=(776, 784))]\n",
      "[token(string=' ', tag=['SP'], offset=(784, 785))]\n",
      "[token(string='field', tag=None, offset=(785, 790))]\n",
      "[token(string=' ', tag=['SP'], offset=(790, 791))]\n",
      "[token(string='variation', tag=None, offset=(791, 800))]\n",
      "[token(string=' ', tag=['SP'], offset=(800, 801))]\n",
      "[token(string='of', tag=None, offset=(801, 803))]\n",
      "[token(string=' ', tag=['SP'], offset=(803, 804))]\n",
      "[token(string='7', tag=None, offset=(804, 805))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(805, 806))]\n",
      "[token(string='T.', tag=None, offset=(806, 808))]\n",
      "[token(string=' ', tag=['SP'], offset=(808, 809))]\n",
      "[token(string='Interestingly,', tag=None, offset=(809, 823))]\n",
      "[token(string=' ', tag=['SP'], offset=(823, 824))]\n",
      "[token(string='a', tag=None, offset=(824, 825))]\n",
      "[token(string=' ', tag=['SP'], offset=(825, 826))]\n",
      "[token(string='sudden', tag=None, offset=(826, 832))]\n",
      "[token(string=' ', tag=['SP'], offset=(832, 833))]\n",
      "[token(string='drop', tag=None, offset=(833, 837))]\n",
      "[token(string=' ', tag=['SP'], offset=(837, 838))]\n",
      "[token(string='of', tag=None, offset=(838, 840))]\n",
      "[token(string=' ', tag=['SP'], offset=(840, 841))]\n",
      "[token(string='resistivity', tag=None, offset=(841, 852))]\n",
      "[token(string=' ', tag=['SP'], offset=(852, 853))]\n",
      "[token(string='curve', tag=None, offset=(853, 858))]\n",
      "[token(string=' ', tag=['SP'], offset=(858, 859))]\n",
      "[token(string='at', tag=None, offset=(859, 861))]\n",
      "[token(string=' ', tag=['SP'], offset=(861, 862))]\n",
      "[token(string='T', tag=None, offset=(862, 863))]\n",
      "[token(string='g', tag=[key(tag='sub', attrs={})], offset=(863, 864))]\n",
      "[token(string=',', tag=None, offset=(864, 865))]\n",
      "[token(string=' ', tag=['SP'], offset=(865, 866))]\n",
      "[token(string='associated', tag=None, offset=(866, 876))]\n",
      "[token(string=' ', tag=['SP'], offset=(876, 877))]\n",
      "[token(string='with', tag=None, offset=(877, 881))]\n",
      "[token(string=' ', tag=['SP'], offset=(881, 882))]\n",
      "[token(string='magnetic', tag=None, offset=(882, 890))]\n",
      "[token(string=' ', tag=['SP'], offset=(890, 891))]\n",
      "[token(string='frustration', tag=None, offset=(891, 902))]\n",
      "[token(string=' ', tag=['SP'], offset=(902, 903))]\n",
      "[token(string='or', tag=None, offset=(903, 905))]\n",
      "[token(string=' ', tag=['SP'], offset=(905, 906))]\n",
      "[token(string='magnetic', tag=None, offset=(906, 914))]\n",
      "[token(string=' ', tag=['SP'], offset=(914, 915))]\n",
      "[token(string='disorder,', tag=None, offset=(915, 924))]\n",
      "[token(string=' ', tag=['SP'], offset=(924, 925))]\n",
      "[token(string='can', tag=None, offset=(925, 928))]\n",
      "[token(string=' ', tag=['SP'], offset=(928, 929))]\n",
      "[token(string='be', tag=None, offset=(929, 931))]\n",
      "[token(string=' ', tag=['SP'], offset=(931, 932))]\n",
      "[token(string='related', tag=None, offset=(932, 939))]\n",
      "[token(string=' ', tag=['SP'], offset=(939, 940))]\n",
      "[token(string='to', tag=None, offset=(940, 942))]\n",
      "[token(string=' ', tag=['SP'], offset=(942, 943))]\n",
      "[token(string='the', tag=None, offset=(943, 946))]\n",
      "[token(string=' ', tag=['SP'], offset=(946, 947))]\n",
      "[token(string='ferromagnetic', tag=None, offset=(947, 960))]\n",
      "[token(string=' ', tag=['SP'], offset=(960, 961))]\n",
      "[token(string='(FM)', tag=None, offset=(961, 965))]\n",
      "[token(string=' ', tag=['SP'], offset=(965, 966))]\n",
      "[token(string='phases', tag=None, offset=(966, 972))]\n",
      "[token(string=' ', tag=['SP'], offset=(972, 973))]\n",
      "[token(string='with', tag=None, offset=(973, 977))]\n",
      "[token(string=' ', tag=['SP'], offset=(977, 978))]\n",
      "[token(string='antiferromagnetic', tag=None, offset=(978, 995))]\n",
      "[token(string=' ', tag=['SP'], offset=(995, 996))]\n",
      "[token(string='antiphase', tag=None, offset=(996, 1005))]\n",
      "[token(string=' ', tag=['SP'], offset=(1005, 1006))]\n",
      "[token(string='boundaries,', tag=None, offset=(1006, 1017))]\n",
      "[token(string=' ', tag=['SP'], offset=(1017, 1018))]\n",
      "[token(string='giving', tag=None, offset=(1018, 1024))]\n",
      "[token(string=' ', tag=['SP'], offset=(1024, 1025))]\n",
      "[token(string='rise', tag=None, offset=(1025, 1029))]\n",
      "[token(string=' ', tag=['SP'], offset=(1029, 1030))]\n",
      "[token(string='to', tag=None, offset=(1030, 1032))]\n",
      "[token(string=' ', tag=['SP'], offset=(1032, 1033))]\n",
      "[token(string='a', tag=None, offset=(1033, 1034))]\n",
      "[token(string=' ', tag=['SP'], offset=(1034, 1035))]\n",
      "[token(string='large', tag=None, offset=(1035, 1040))]\n",
      "[token(string=' ', tag=['SP'], offset=(1040, 1041))]\n",
      "[token(string='negative', tag=None, offset=(1041, 1049))]\n",
      "[token(string=' ', tag=['SP'], offset=(1049, 1050))]\n",
      "[token(string='magnetoresistance', tag=None, offset=(1050, 1067))]\n",
      "[token(string=' ', tag=['SP'], offset=(1067, 1068))]\n",
      "[token(string='(∼67%)', tag=None, offset=(1068, 1074))]\n",
      "[token(string=' ', tag=['SP'], offset=(1074, 1075))]\n",
      "[token(string='at', tag=None, offset=(1075, 1077))]\n",
      "[token(string=' ', tag=['SP'], offset=(1077, 1078))]\n",
      "[token(string='45', tag=None, offset=(1078, 1080))]\n",
      "[token(string='\\u2009', tag=['SP'], offset=(1080, 1081))]\n",
      "[token(string='K.', tag=None, offset=(1081, 1083))]\n",
      "[token(string='\\n', tag=['SP'], offset=(1083, 1084))]\n",
      "1 302\n",
      "[token(string='I.', tag=None, offset=(1084, 1086))]\n",
      "[token(string=' ', tag=['SP'], offset=(1086, 1087))]\n",
      "[token(string='INTRODUCTION', tag=None, offset=(1087, 1099))]\n",
      "[token(string='\\n', tag=['SP'], offset=(1099, 1100))]\n",
      "2 4\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib2 import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import splitword  # modules for tokenization\n",
    "\n",
    "token = namedtuple('token', 'string tag offset')  # type(elem): string, type(tags): list\n",
    "keytag = namedtuple('key', 'tag attrs')  # type(elem): string, type(tags): list\n",
    "\n",
    "class ReadFile:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.textfile = kwargs['fdir'] / kwargs['textfile']\n",
    "        with self.textfile.open(encoding='utf-8') as f:\n",
    "            self.doc = f.read()\n",
    "        self.section = kwargs['fdir'] / kwargs['sectionfile']\n",
    "        self.tag = kwargs['fdir'] / kwargs['tagfile']\n",
    "        self.df_section = pd.read_csv(self.section)\n",
    "        self.df_tag = pd.read_csv(self.tag)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.doc\n",
    "\n",
    "    @property\n",
    "    def sectionlist(self):\n",
    "        return self.df_section\n",
    "\n",
    "    @property\n",
    "    def taglist(self):\n",
    "        return self.df_tag\n",
    "\n",
    "    def rmsections(self, sect):\n",
    "        secs = self.df_section[self.df_section['sec_title'].str.match(re.compile(sect, re.I))]\n",
    "        newdoc = self.doc\n",
    "        tups = []\n",
    "        rm_secs = []\n",
    "        rm_tags = []\n",
    "        for sec in secs.itertuples():\n",
    "            s, e = sec.start, sec.end\n",
    "            tups.append([s, e])\n",
    "            newdoc = newdoc.replace(self.doc[s:e], '', 1)\n",
    "\n",
    "            sloc = self.df_section[(self.df_section.start >= s) & (self.df_section.end <= e)].index\n",
    "            rm_secs += [i for i in sloc]\n",
    "\n",
    "            tloc = self.df_tag[(self.df_tag.start >= s) & (self.df_tag.end <= e)].index\n",
    "            if len(tloc) > 0:\n",
    "                rm_tags += [i for i in tloc]\n",
    "\n",
    "        for tup in tups[::-1]:  # subtract from the end of lists\n",
    "            s, e = tup\n",
    "            offset = e - s\n",
    "            self.df_section.loc[self.df_section.start >= s, 'start'] -= offset\n",
    "            self.df_section.loc[self.df_section.end >= e, 'end'] -= offset\n",
    "            self.df_tag.loc[self.df_tag.start >= s, 'start'] -= offset\n",
    "            self.df_tag.loc[self.df_tag.end >= e, 'end'] -= offset\n",
    "\n",
    "        self.df_section.drop(rm_secs)\n",
    "        self.df_tag.drop(rm_tags)\n",
    "        return newdoc\n",
    "\n",
    "    \n",
    "    def process(self, newdoc):\n",
    "        paratextlist = []\n",
    "        for i, sec in enumerate(self.df_section.itertuples()):\n",
    "            part_tag = self.df_tag.loc[(self.df_tag.start >= sec.start)  & (self.df_tag.end <= sec.end)]\n",
    "            st, se = sec.start, sec.end\n",
    "            textlist = []\n",
    "            for j, tag in enumerate(part_tag.itertuples()):\n",
    "                s, e = tag.start, tag.end\n",
    "                keys = [keytag(list(it.keys())[0], list(it.values())[0]) \\\n",
    "                        for it in eval(tag.taglist)]\n",
    "                if newdoc[st : s]:\n",
    "                    textlist += [token(newdoc[st : s], None, (st, s))]\n",
    "                textlist += [token(newdoc[s : e], keys, (s, e))]\n",
    "                st = e\n",
    "            if newdoc[e : sec.end]:\n",
    "                textlist += [token(newdoc[e : sec.end], None, (e, sec.end))]\n",
    "                e = sec.end\n",
    "                \n",
    "            paratextlist.append(textlist)\n",
    "        return paratextlist  \n",
    "\n",
    "\n",
    "class TokenList:\n",
    "    def __init__(self, diclist):\n",
    "        self.diclist = diclist\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.diclist[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.diclist)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ''.join([list(dic.values())[0] for dic in self.diclist])\n",
    "\n",
    "    def getlist(self, attr):\n",
    "        return [list(dic.values())[0] for dic in self.diclist if attr in dic.keys()]\n",
    "\n",
    "    def getkeys(self):\n",
    "        return set([list(dic.keys())[0] for dic in self.diclist])\n",
    "\n",
    "    def splitby(self, mod, comms):\n",
    "        textlist = self.diclist\n",
    "        for command, attr in comms.items():\n",
    "            newlist = []\n",
    "            for n, dic in enumerate(textlist):\n",
    "                newlist += eval(mod + '.' + command)(dic)\n",
    "            textlist = newlist[:]\n",
    "        return textlist\n",
    "\n",
    "\n",
    "def setparagraph(tokenlist):\n",
    "    paragraphtexts = []\n",
    "    indexes = [ii for ii, w in enumerate(tokenlist) if list(w.values())[0] == '\\n']\n",
    "    s = 0\n",
    "    for index in indexes:\n",
    "        e = index + 1\n",
    "        text = ''.join([''.join(list(w.values())) for w in tokenlist[s:e]])\n",
    "        if text != '\\n':\n",
    "            paragraphtexts.append(text)\n",
    "        s = e\n",
    "    return paragraphtexts\n",
    "\n",
    "\n",
    "def calcoffset(tokenlist, offset):\n",
    "    entity_dic = {\n",
    "        'TK': '_',\n",
    "        'PR': '_',\n",
    "        'SR': '_',\n",
    "        'PN': '_',\n",
    "        'BL1': '_',\n",
    "        'BL2': '_',\n",
    "        'LF': '_',\n",
    "        'SP': '_',\n",
    "        'TK1': 'compound',\n",
    "        'TK2': 'compound',\n",
    "        'TK3': 'compound',\n",
    "        'TK4': 'compound',\n",
    "        'IN1': 'compound',\n",
    "        'IN2': 'compound',\n",
    "        'IN3': 'compound',\n",
    "        'SUB': 'subscript',\n",
    "        'SUP': 'superscript',\n",
    "        'ITL': 'italic',\n",
    "        'REF': 'reference',\n",
    "        '_': '_'\n",
    "    }\n",
    "    dic_token = {}\n",
    "    ipar = 1  # for dic_token.keys()\n",
    "    itok = 1  # for dic_token.keys()\n",
    "    ient = 1  # counter for entities\n",
    "    for i in range(len(tokenlist)):\n",
    "        value = list(tokenlist[i].values())[0]\n",
    "        ent = list(tokenlist[i].keys())[0]\n",
    "        entity = entity_dic.get(ent)\n",
    "        if entity and entity != '_':\n",
    "            if entity_dic.get(list(tokenlist[i - 1].keys())[0]) != entity_dic.get(\n",
    "                    ent):  # different entity case from previous\n",
    "                ient += 1\n",
    "            entity += f'[{ient}]'\n",
    "\n",
    "        if i < len(tokenlist) - 1 and 'LF' not in tokenlist[i].keys() and 'SP' in tokenlist[i + 1].keys():\n",
    "            s = offset\n",
    "            e = offset + len(value)\n",
    "            dic_token[ipar, itok] = (s, e, value, entity)  # words without space\n",
    "            e += len(list(tokenlist[i + 1].values())[0])  # count for space\n",
    "            offset = e\n",
    "            itok += 1\n",
    "        elif 'LF' in tokenlist[i].keys():\n",
    "            dic_token[ipar, itok] = value\n",
    "            offset += len(value)\n",
    "            ipar += 1\n",
    "            itok = 1\n",
    "        elif 'SP' not in tokenlist[i].keys():\n",
    "            s = offset\n",
    "            e = offset + len(value)\n",
    "            dic_token[ipar, itok] = (s, e, value, entity)\n",
    "            offset = e\n",
    "            itok += 1\n",
    "    return dic_token\n",
    "\n",
    "\n",
    "class OutFile:\n",
    "\n",
    "    def __init__(self, tokenlist, **kwargs):\n",
    "        self.tokenlist = tokenlist\n",
    "        self.outfile = kwargs['webannotsv']\n",
    "\n",
    "    def outputtsv(self, orig_paragraph):\n",
    "\n",
    "        fw = open(self.outfile, 'w', encoding='utf-8')\n",
    "        tsv_text = '#FORMAT=WebAnno TSV 3.2\\n'\n",
    "        tsv_text += '#T_SP=webanno.custom.Xml|xml_tag\\n\\n\\n'\n",
    "\n",
    "        paragraph_texts = setparagraph(self.tokenlist)\n",
    "        dic_token = calcoffset(self.tokenlist, offset=0)\n",
    "\n",
    "        # check paragraphs\n",
    "        matcher = SequenceMatcher()\n",
    "        matcher.set_seq1(paragraph_texts)\n",
    "        matcher.set_seq2(orig_paragraph)\n",
    "        assert matcher.quick_ratio() == 1, \"Paragraphs don't match!\"\n",
    "\n",
    "        for i, paragraph_text in enumerate(paragraph_texts):\n",
    "            tsv_text += f'#Text={paragraph_text}'\n",
    "            for keys, values in dic_token.items():\n",
    "                if keys[0] == i + 1:\n",
    "                    if values == '\\n':\n",
    "                        tsv_text += values\n",
    "                    else:\n",
    "                        tsv_text += ('{}-{}\\t{}-{}\\t{}\\t{}\\t\\n'.format(*keys, *values))\n",
    "\n",
    "        print(tsv_text.rstrip('\\n\\n'), file=fw)\n",
    "\n",
    "\n",
    "# main\n",
    "filelist = {\n",
    "    'fdir': Path('./xml2text'),\n",
    "    'textfile': '10.1063_1.5004600_fulltext_20190516.txt',\n",
    "    'sectionfile': '10.1063_1.5004600_section_offset_20190516.csv',\n",
    "    'tagfile': '10.1063_1.5004600_xmltag_offset_20190516.csv',\n",
    "    'webannotsv': '10.1063_1.5004600_webanno.tsv'\n",
    "}\n",
    "\n",
    "doc = ReadFile(**filelist)\n",
    "\n",
    "section = r'TABLE(.+?)-body'  # remove sections (Table body)\n",
    "new_doc = doc.rmsections(section)\n",
    "para_text_list = doc.process(new_doc)\n",
    "\n",
    "# tokenize\n",
    "module = 'splitword'\n",
    "commands = {\n",
    "    'splitbyspace': 'TX',\n",
    "#     'splitbypunct': 'TK',\n",
    "#     'splitbyblacket2': 'TK',\n",
    "    'splitbyinfix': 'TK',\n",
    "#     'splitbyprefix2': 'TK',\n",
    "#     'splitbysurfix2': 'TK'\n",
    "}\n",
    "\n",
    "new_para_text_list = []\n",
    "for i, text_list in enumerate(para_text_list[:3]):\n",
    "    tok = TokenList(text_list)\n",
    "    text_list = tok.splitby(module, commands)\n",
    "    print(i, len(text_list))\n",
    "    new_para_text_list.append(text_list)\n",
    "\n",
    "# output\n",
    "out = OutFile(text_list, **filelist)\n",
    "# out.outputtsv(doc.newparagraph(new_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ReadFile(**filelist)\n",
    "set(a.taglist.taglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "paragraph_texts = setparagraph(prefix_tokenlist)\n",
    "para = ''.join(paragraph_texts).rstrip().split('\\n')\n",
    "orig_parahgraph_texts = new_doc.rstrip().split('\\n')\n",
    "d = difflib.Differ()\n",
    "diff = d.compare(para, orig_parahgraph_texts)\n",
    "print('\\n'.join(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, token) for i, token in enumerate(para_text_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
